---
title: Analysis 
description:
toc: true
featuredVideo: 
featuredImage: https://learn.g2.com/hubfs/Imported%20sitepage%20images/1ZB5giUShe0gw9a6L69qAgsd7wKTQ60ZRoJC5Xq3BIXS517sL6i6mnkAN9khqnaIGzE6FASAusRr7w=w1439-h786.png
draft: false
---

## Motivation
Most of our teammates are international students from China and Korea. We found an article that the number of international students in the US unprecedentedly still increased during 2019 to 2021. Widespread travel restrictions do not appear to have had a large impact on freshmen applying to U.S. universities from outside the U.S. We wanted to see what influences international students to choose certain institutions in the United States.

One of our teammates, Yawei suggested that the top three considerations were: safety, ranking of the university in my field, and the “dynamics” of the campus. The percentage of the international student body would fall under the third category. However, while there has been quite convenient data on the first two categories, the judgment of the third was more based on gut or anecdotes. The rankings on US News, for example, are a weighted composite score of many aspects, but we were not sure whether the international student body is one of the criteria (probably not, but I believe there is correlation there). Therefore, we considered that we could explore more about this potential correlation (x=% of international students, y=rankings) while trying to add in other interesting and relevant information, such as the nationalities of international students, crime rate of the city, etc

The “main” research question is to explore the relationship between percentage of international student population (x-axis) and the ranking of US universities (y-axis). We can then add more interesting complexities/layers to the basic graph through **“group_by”, “color”, “fill”, “facet”** and different “geoms_plots” using variables such as nationalities, number of (top xx) universities in the city, crime rates of the city, location of the university (on a US map), and ranking type (e.g., undergraduate, graduate cs, graduate education, graduate business, graduate law). 

<br>
We also wanted to answer the questions below: 
<br/>
**1.** How has the number of international students in the US changed over time?  (x= school years, y= international students’ population)
<br/>
**2.** New enrollment percentage of international students (x= school years, y= international students’ population % change)
<br/>
**3.** How do new enrollment trends vary by academic level? 
<br/>
**4.** Breakdown of international students by the US region they live (2021 vs 2016)

## Breadth and Depth of DA
Our group’s target variables are the international student population (ISPo) and proportion (ISPr) in US colleges, and we aim to identify the potential factors that correlate to and predict ISPo and ISPr. 

<br>
In the dataset, the features indicate ISPo and ISPr are following:
<br>
**ISPo**: international_students_max <br/>
**ISPr**: international_students_natlib

## Early Data Analysis
In our earlier data analysis (Blog Post 4), we examined the relationship between the dependent variable ISPr and the independent variable, an additive indicator reflecting the number of “Alumni of an institution winning Nobel Prizes and Fields Medals” and “Papers indexed in Science Citation Index-Expanded and Social Science Citation Index”. We did this by making predictions with 7 regression models (logistic, quasipolynomial, Gaussian, Poisson, Inverse Gaussian, Gamma) after shuffling and splitting the data with a 70-30 ratio for the train and test sets. By evaluating the AIC and fisher score, we selected the inverse gaussian glm as the best fit of all 7 models. Below we display our linear regression, logistic regression, and inverse gaussian regression.   
<br>
One main limitation is that we naively selected one feature, a linear combination of variables on notable alumni with high research and publications, for modelling. Below in the Feature Extraction section, we remedy this problem by experimenting on all predictor variables to select the most promising features to be learned.    
<br>
Another main limitation is that we have missing and incorrect data over 300 observations. In particular, we had issues with variables city, state, city_population, city_population_density. We resolved this by imputing the missing values and manually correcting by using US Census and universities' published data as discussed in our Data Page.  

## Feature Extraction
We extract the most significant independent variables to improve model performance using two methods: 
correlation matrix for continuous predictors and logistic regression for categorical predictors.   
<br>
Since we are solving a regression problem, our output is a continuous variable. After data cleaning, we have 35 independent variables concerning every observed college. They are split into 20 continuous variables and 15 categorical variables.    

### Continuous Predictors  
To explore continuous variables’ correlations with the target variables, we have used a correlation matrix.
```{r, echo=FALSE, message = FALSE}
 suppressWarnings({
 library(tidyverse)
 library(caTools)
 library(ggplot2)
 library(ggcorrplot)
 library(reshape2) 
 library(GGally)
 })

 get_upper_tri <- function(cormat){
   cormat[lower.tri(cormat)]<- NA
   return(cormat)
 }

 reorder_cormat <- function(cormat){
   dd <- as.dist((1-cormat)/2)
   hc <- hclust(dd)
   cormat <-cormat[hc$order, hc$order]
 }

 suppressWarnings({
 clean_data <- read.csv(here::here("dataset/prev_data.csv"))
 num_data <- clean_data[,unlist(lapply(clean_data, is.numeric))]
 cormat <- round(cor(num_data, use = "complete.obs"), 2)
 cormat <- reorder_cormat(cormat)
 })


 suppressWarnings({
 upper_tri <- get_upper_tri(cormat)
 melted_cormat <- melt(upper_tri, na.rm = TRUE)
 ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
   geom_tile(color = "white")+
   scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                        midpoint = 0, limit = c(-1,1), space = "Lab", 
                        name="Pearson\nCorrelation") +
   theme_minimal()+ # minimal theme
   theme(axis.text.x = element_text(angle = 45, vjust = 1,size = 6, hjust = 1)) + coord_fixed()
 })

 suppressWarnings({
 ind_num <- melted_cormat[(melted_cormat$Var1 == "international_students_max" & melted_cormat$Var2 != "international_students_max" & abs(melted_cormat$value) >=0.4 ), ]$Var2
 ind_prop <- melted_cormat[(melted_cormat$Var1 == "international_students_natlib" & melted_cormat$Var2 != "international_students_natlib" & abs(melted_cormat$value) >=0.4 ), ]$Var2
 ggheatmap
 })
```
<br>

We identified pairs of variables with high positive or negative correlations and selected numerical features that have an absolute value of correlation coefficient greater than 0.4 with respect to ISPo and ISPr. 

#### For ISPo, the relevant numerical features are:  
1. ~~The number of international students in Opendoors (international_students_opendoors)~~
2. Total student enrollment number (enrollment_top120enrollment)
3. City population (city_population)
4. ~~The number of international students in QS (international_students_qs)~~
5. ~~Percentage of international students (international_students_natlib)~~
6. City population density (city_population_density)    

<br>
From the relevant features, we crossed out features: "international_students_opendoors", "international_students_qs", since we used opendoors and qs feature to create ISPo (international_students_max) column. Also, "international_students_natlib" is the feature showing ISPr and we already have feature extraction down below, we removed that as well

<br>
Below is the scatter plot and correlation matrix of ISPo related data to check how the dots are scattered and whether two variables have strong/moderate/weak positive/negative relations. 
<br>

```{r message = FALSE, echo = FALSE, warning = FALSE}
ISPo_data = num_data[c("international_students_max", "enrollment_top120enrollment", "city_population", "city_population_density")]

ggpairs(ISPo_data, upper = list(continuous = wrap("points", alpha = 0.5, size = 0.3)),
        lower = list(continuous = wrap('cor', size = 4))) + theme(axis.text = element_text(size = 4))+ theme_grey(base_size = 6.5)
```
<br>
Overall, the points in the scatterplots are very scattered and clustered and shows a weak positive relation with ISPo(international_student_max). Here, the total student enrollment number(enrollment_top120enrollment) shows the highest correlation with USPo, however, the data has a lot of nulls which lead to consider removing the feature. 


#### For ISPr, the relevant numerical features are:  
1. City population density (city_population_density)  
2. The number of faculty or academic staff at the university (faculty_count_qs)  

<br>   

Below is the scatter plot and correlation matrix of ISPr related data to check how the dots are scattered and whether two variables have weak/moderate positive/negative relations. 
<br>

```{r message = FALSE, echo = FALSE, warning = FALSE}
ISPo_data = num_data[c("international_students_natlib", "city_population_density", "faculty_count_qs")]

ggpairs(ISPo_data, upper = list(continuous = wrap("points", alpha = 0.5, size = 0.3)),
        lower = list(continuous = wrap('cor', size = 4))) + theme(axis.text = element_text(size = 4))
```
<br>
Here, city population density (city_population_density) feature shows the highest correlation with ISPr(international_students_natlib). Both of the feature shows the low positive relation with ISPr.

<br>
### Categorical Predictors
<b>Note</b>: Before using logistic regression, we omit variable "university" because it is unique for all observations.    
<br>
We build logistic regression models with single categorical variable predictors and pairs of categorical variable predictors. For readability, we omit the models with paired predictors. However, we document all performances in the R code block. We select variables from models using the Akaike information criterion (AIC) metric. Below, we provide the summary of the logistic regression model with response variable international student population and independent variable research output.
```{r, echo=FALSE, message = FALSE}
suppressWarnings({
prev_data <- read.csv(here::here("dataset/prev_data.csv"))

glm_model_city <- glm(international_students_natlib~city, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_city)
#AIC: 425.04
glm_model_state <- glm(international_students_natlib~state, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_state)
#AIC: 130.42
#type3_natlib is not effective
glm_model_selectivity <- glm(international_students_natlib~selectivity, data=prev_data, family =binomial(), na.action=na.omit )
summary(glm_model_selectivity)
#AIC: 34.986
# world_rank_qs, national_rank_arwu, type1_all,  is not effective
glm_model_world_rank_cat_arwu <- glm(international_students_natlib~world_rank_cat_arwu, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_world_rank_cat_arwu)
#AIC: 29.971
#No pairs perform better
glm_model_world_rank_qs <- glm(international_students_natlib~world_rank_qs, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_world_rank_qs)
#AIC: 43.828
#type2_carnegie, type4_carnegie, enrprofile2021_carnegie, locale_carnegie does well
glm_model_national_rank_arwu <- glm(international_students_natlib~national_rank_arwu, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_national_rank_arwu)
#AIC: 84.028
#type2_carnegie, enrprofile2021_carnegie, locale_carnegie does well
glm_model_type1_all <- glm(international_students_natlib~type1_all, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_type1_all)
#AIC: 49.552
# enrprofile2021_carnegie, locale_carnegie,research_output_qs does well
glm_model_type2_carnegie <- glm(international_students_natlib~type2_carnegie, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_type2_carnegie)
#AIC: 46.749
#size_qs, research_output_qs does well
glm_model_type3_natlib <- glm(international_students_natlib~type3_natlib, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_type3_natlib)
#AIC:  52.316
#type4_carnegie, enrprofile2021_carnegie, locale_carnegie, size_qs, research_output_qs does well
glm_model_type4_carnegie<- glm(international_students_natlib~type4_carnegie, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_type4_carnegie)
# AIC: 48.755
#size_qs, research_output_qs does well
glm_model_enrprofile2021_carnegie<- glm(international_students_natlib~enrprofile2021_carnegie, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_enrprofile2021_carnegie)
#AIC: 46.748
#size_qs, research_output_qs does well
glm_model_locale_carnegie<- glm(international_students_natlib~locale_carnegie, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_locale_carnegie)
#AIC: 46.74
#size_qs, research_output_qs improves
glm_model_size_qs<- glm(international_students_natlib~size_qs+research_output_qs, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_size_qs)
#AIC: 28.158
#research_output_qs does not improve
glm_model_research_output_qs<- glm(international_students_natlib~research_output_qs, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_research_output_qs)
#AIC: 24.169
glm_model_city_max <- glm(as.factor(international_students_max)~city+type1_all, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_city_max)
#AIC: 491.05
#All pairs  are good
glm_model_state_max <- glm(as.factor(international_students_max)~state, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_state_max)
#AIC: 100.9
#world_rank_cat_arwu, national_rank_arwu, type1_all is not effective
glm_model_selectivity_max <- glm(as.factor(international_students_max)~selectivity, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_selectivity_max)
#AIC: 17.844
# type3_natlib, locale_carnegie, research_output_qs is effective
glm_model_world_rank_cat_arwu_max <- glm(as.factor(international_students_max)~world_rank_cat_arwu, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_world_rank_cat_arwu_max)
#AIC:22.279
#type4_carnegie perform better
glm_model_world_rank_qs_max <- glm(as.factor(international_students_max)~world_rank_qs, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_world_rank_qs_max)
#AIC: 32.477
#type3_natlib, enrprofile2021_carnegie,  does well
glm_model_national_rank_arwu_max <- glm(as.factor(international_students_max)~national_rank_arwu, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_national_rank_arwu_max)
#AIC: 84.279
#type2_carnegie, type4_carnegie, enrprofile2021_carnegie,  locale_carnegie,  does well
glm_model_type1_all_max <- glm(as.factor(international_students_max)~type1_all, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_type1_all_max)
#AIC: 17.922
# type3_natlib, size_qs, research_output_qs does well
glm_model_type2_carnegie_max <- glm(as.factor(international_students_max)~type2_carnegie, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_type2_carnegie_max)
#AIC: 17.922
#type3_natlib, research_output_qs does well
glm_model_type3_natlib_max <- glm(as.factor(international_students_max)~type3_natlib, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_type3_natlib_max)
#AIC: 12.835
# size_qs, research_output_qs does well
glm_model_type4_carnegie_max<- glm(as.factor(international_students_max)~type4_carnegie, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_type4_carnegie_max)
# AIC: 17.941
# research_output_qs does well
glm_model_enrprofile2021_carnegie_max<- glm(as.factor(international_students_max)~enrprofile2021_carnegie, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_enrprofile2021_carnegie_max)
#AIC: 17.773
#size_qs, research_output_qs does well
glm_model_locale_carnegie_max<- glm(as.factor(international_students_max)~locale_carnegie, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_locale_carnegie_max)
#AIC: 18.067
glm_model_size_qs_max<- glm(as.factor(international_students_max)~size_qs, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_size_qs_max)
#AIC: 17.301
#research_output_qs does not improve
glm_model_research_output_qs_max<- glm(as.factor(international_students_max)~research_output_qs, data=prev_data, family = binomial(), na.action=na.omit )
summary(glm_model_research_output_qs_max)
#AIC: 16.015
#______
 })

```
<br>

#### For ISPo, the relevant categorical features are:  
1. Quality of research (research_output_qs)   
2. Size of the university (size_qs)   
3. National or Liberal Arts (type3_natlib)   
4. Carnegie Basic Classification (type4_carnegie)   
5. Located State in the US (state)   
6. Selectivity (selectivity)   
7. World ranking from ARWU (world_rank_cat_arwu)  

<br>

#### Boxplots of ISPo and Selected Relevant Categorical Variables
```{r message = FALSE, echo = FALSE, warning = FALSE}
library(patchwork)
load(here::here("dataset", "clean_data.RData"))

ISPo_Cat <- clean_data[c("international_students_max", "research_output_qs","size_qs", "type3_natlib", "type4_carnegie", "world_rank_cat_arwu", "selectivity", "state")] 

data10 <- clean_data[c("international_students_max", "research_output_qs")] %>% drop_na()
data10_melt <- melt(data10, id = "research_output_qs")
bp1<- ggplot(data10_melt, aes(x = variable, y = value, color = factor(research_output_qs))) + geom_boxplot() + theme_bw() + theme(axis.text.x=element_blank()) + labs(x = "", y = "ISPo", title = "research_output_qs feature") + scale_color_discrete("Research Outputs")

data11 <- clean_data[c("international_students_max", "size_qs")] %>% drop_na()
data11_melt <- melt(data11, id = "size_qs")
bp2<- ggplot(data11_melt, aes(x = variable, y = value, color = factor(size_qs))) + geom_boxplot() + theme_bw() + theme(axis.text.x=element_blank()) + labs(x = "", y = "ISPo", title ="size_qs feature") + scale_color_discrete("School Sizes")

data15 <- clean_data[c("international_students_max", "selectivity")] %>% drop_na()
data15_melt <- melt(data15, id = "selectivity")
bp6<-ggplot(data15_melt, aes(x = variable, y = value, color = factor(selectivity))) + geom_boxplot() + theme_bw() + theme(axis.text.x=element_blank()) + labs(x = "", y = "ISPo", title = "selectivity feature") + scale_color_discrete("School Selectivities")

bp1 + bp2 
bp6 
```

These are the boxplots that shows the most relevant relation with ISPo. Here, the boxplots of the "research_output_qs" and "size_qs" features show the obvious positive relations with ISPo follows by the the feature. "selectivity" feature seems to show weak relation with ISPo, however, the mean of the boxplot seems increasing - we can say "weak positive relation"

<br>
To make the state variable viewable we tried to create the feature "region" using "state" by grouping states into four regions(Midwest, West, South, and Northeast) and then we made boxplots with region variable
<br>
```{r message = FALSE, echo = FALSE, warning = FALSE}
region <- initialize(.Object = "ANY")

#IL, IN, IA, KS, MI, MN, MO, NE, ND, OH, SD, WI
region[clean_data$state=="IL" | clean_data$state=="IN" | clean_data$state=="IA" | clean_data$state=="KS" | clean_data$state=="MI" | clean_data$state=="MN" | clean_data$state=="MO" | clean_data$state=="NE" | clean_data$state=="ND" | clean_data$state=="OH" | clean_data$state=="SD" | clean_data$state=="WI"] <- "Midwest"

#CO, ID, MT, NV, UT, WY, AK, CA, HI, OR, WA
region[clean_data$state=="CO" | clean_data$state=="ID" | clean_data$state=="MT" | clean_data$state=="NV" | clean_data$state=="UT" | clean_data$state=="WY" | clean_data$state=="AK" | clean_data$state=="CA" | clean_data$state=="HI" | clean_data$state=="OR" | clean_data$state=="WA"] <- "West"

#MD, DE,VA, WV, KY, TN, NC, SC, FL, GA, AL, MS, LA, AK, TX, OK
region[clean_data$state=="MD" | clean_data$state=="DE" | clean_data$state=="VA" | clean_data$state=="WV" | clean_data$state=="KY" | clean_data$state=="TN" | clean_data$state=="NC" | clean_data$state=="SC" | clean_data$state=="FL" | clean_data$state=="GA" | clean_data$state=="AL" | clean_data$state=="MS" | clean_data$state=="LA" | clean_data$state=="AK" | clean_data$state=="TX" | clean_data$state=="OK"] <- "South"

#CT, NH, PA, ME, MA, NJ, NY, RI, VT
region[clean_data$state=="CT" | clean_data$state=="NH" | clean_data$state=="PA" | clean_data$state=="ME" | clean_data$state=="MA" | clean_data$state=="NJ" | clean_data$state=="NY" | clean_data$state=="RI" | clean_data$state=="VT"] <- "Northeast"
clean_data$region <- region

data17 <- clean_data[c("international_students_max", "region")] %>% drop_na()
data17_melt <- melt(data17, id = "region")
ggplot(data17_melt, aes(x = variable, y = value, color = factor(region))) + geom_boxplot() + theme_bw() + theme(axis.text.x=element_blank()) + labs(x = "", y = "ISPo") + scale_color_discrete("Regions")

```


#### For ISPr, the relevant categorical features are:  
1. Quality of research (research_output_qs)   
2. Size of the university (size_qs)   
3. Urban-centric locale (locale_carnegie)
4. Enrollment profile classification (enrprofile2021_carnegie)  
5. Year and size classification (type2_carnegie)  
6. Carnegie Basic Classification (type4_carnegie)
7. World ranking from ARWU (world_rank_cat_arwu)    
8. Selectivity (selectivity)    
9. Located State in the US (state)   



#### Boxplots of ISPr and Relevant Categorical Variables
```{r message = FALSE, echo = FALSE, warning = FALSE}
data1 <- clean_data[c("international_students_natlib", "research_output_qs")] %>% drop_na()
data1_melt <- melt(data1, id = "research_output_qs")
bp11<- ggplot(data1_melt, aes(x = variable, y = value, color = factor(research_output_qs))) + geom_boxplot() + theme_bw() + theme(axis.text.x=element_blank()) + labs(x = "", y = "ISPr", title="research_output_qs") + scale_color_discrete("Research Outputs")

data2 <- clean_data[c("international_students_natlib", "size_qs")] %>% drop_na()
data2_melt <- melt(data2, id = "size_qs")
bp12<- ggplot(data2_melt, aes(x = variable, y = value, color = factor(size_qs))) + geom_boxplot() + theme_bw() + theme(axis.text.x=element_blank()) + labs(x = "", y = "ISPr", title = "size_qs") + scale_color_discrete("School Sizes")

data8 <- clean_data[c("international_students_natlib", "selectivity")] %>% drop_na()
data8_melt <- melt(data8, id = "selectivity")
bp18<-ggplot(data8_melt, aes(x = variable, y = value, color = factor(selectivity))) + geom_boxplot() + theme_bw() + theme(axis.text.x=element_blank()) + labs(x = "", y = "ISPr", title = "selectivity") + scale_color_discrete("School Selectivities")

bp11 + bp12 
bp18 
```
Here, you can see the "research_output_qs" shows the obvious positive relation with ISPr. Not like ISPo, "size_qs" and ISPr doesn't show the obvious increasing pattern, but the boxplots are getting larger from small to large, meaning that the internation student rate varies in institution with larger campus. The selectivity also shows the same pattern. 


<br>
Same as ISPo, we made boxplots with "region" variable, which is a feature made with "state"

```{r message = FALSE, echo = FALSE, warning = FALSE}
data18 <- clean_data[c("international_students_natlib", "region")] %>% drop_na()
data18_melt <- melt(data18, id = "region")
ggplot(data18_melt, aes(x = variable, y = value, color = factor(region))) + geom_boxplot() + theme_bw() + theme(axis.text.x=element_blank()) + labs(x = "", y = "ISPr") + scale_color_discrete("Regions")


```

### Finalized Features
Since our data is sparse, if we select certain variables for our predictors, then many college observations are dropped. Thereby, for predicting both international student population and proportion, out of our initial 35 dependent variables, we select the features:   
1. city_population  
2. city_population_density  
3. selectivity  
4. size_qs  
5. research_output_qs  


#### The following modeling section exclusively addresses our data analysis.

## Modeling

### Initial Modeling

For our first iteration of modeling, we use the dataset with 35 independent variables and only consider models with numerically extracted features: faculty_count_qs, city_population, city_population_density.
To obtain our train and test datasets, we have performed an <b>70-30 random shuffle-split</b>. We have 53 examples in the train set and 23 examples in test set. Then, we model ISPo with linear and logistic regression. 

```{r, echo=FALSE, message = FALSE}
suppressWarnings({
clean_data <- read.csv(here::here("dataset/prev_data.csv"))
processed_data <- na.omit(data.frame(y = clean_data$international_students_natlib,
                   x1 = clean_data$faculty_count_qs,
                   x2= clean_data$city_population,
                   x3= clean_data$city_population_density
                   ) )
set.seed(2022)
split <- sample.split(processed_data, SplitRatio = 0.7)
dt = sort(sample(nrow(processed_data), nrow(processed_data)*.7))
train<-processed_data[dt,]
test<-processed_data[-dt,]
})

suppressWarnings({ 
linear_model <- lm(y~x1+x2+x3, data=train)
summary(linear_model)

linear_result<- ggplot(test,aes(x1+x2+x3, y)) +
  geom_point() +
  geom_smooth(method = 'loess', formula = 'y ~ x', se=TRUE, color='turquoise4') +
  theme_minimal() +
  labs(x='Faculty count, city population and density factor (additive)', y='International Student Proportions', title='Linear Regression Plot') +
  theme(plot.title = element_text(hjust=0.5, size=15, face='bold')) 

y_pred_linear_model <- predict(linear_model, test, type="response")
errors_linear_model <- test$y - y_pred_linear_model
mae_linear_model <- mean(abs(na.omit(errors_linear_model)))
#Only considers 23 observations
})

suppressWarnings({
logistic_model <- glm(y~x1+x2+x3, data=train, family = binomial()) #Logistic 1
summary(logistic_model)

y_predict_logit <- data.frame(x1 = test$x1,x2 = test$x2, x3 = test$x3, y =predict(logistic_model, test, type = "response"))

logit_result<- ggplot(test, aes(x1+x2+x3, y)) + geom_point() + geom_smooth(data = y_predict_logit, method = 'loess', formula = 'y ~ x', se=FALSE, color='red4', alpha = 0.1)+
  theme_minimal() +
  labs(x='Faculty count, city population and density factor (additive)', y='International Student Proportions', title='Logistic Regression') +
  theme(plot.title = element_text(hjust=0.5, size=15, face='bold')) 
logit_result

y_pred_logistic_model <- predict(logistic_model, test, type="response")
errors_logistic_model <- test$y - y_pred_logistic_model
mae_logistic_model <- mean(abs(na.omit(errors_logistic_model)))
})
#Only uses 23 observations to test
linear_result
logit_result
print("Mean Absolute Error of Linear Regression for ISPr:")
mae_linear_model
print("Mean Absolute Error of Logistic Regression for ISPr:")
mae_logistic_model
```
While linear regression has a mean absolute error of *0.03342281* and logistic regression has a mean absolute error of  *0.03372792* suggests the models have 'low' error, the modelling is not robust since the train and test set are small, only contains 53 and 23 observations, respectively. We are interested in experimenting with more test data and using a combination of continuous and categorical predictors for the final modeling. 

### Finalized Modeling

To improve upon our last iteration of modeling, we have imputed city population and density data for all university observations, which enables more available training and test data. We also have selected highly-correlated predictors for this regression problem to improve model performance.    
<br>
To obtain our train and test datasets, we have performed an <b>80-20 random shuffle-split</b>. We have 352 examples in the train set and 89 examples in test set.    
<br>
Then using our pruned features, we model ISPo and ISPr with <b>logistic regression, stepwise linear regression with forward and backward selection, and inverse gaussian regression</b>.   
<br>
Below, we present the logistic regression and inverse gaussian regression models and model evaluations.   
```{r, echo=FALSE, message = FALSE, warning= FALSE}
suppressWarnings({
library(tidyverse)
library(caTools)
library(ggplot2)
library(ggcorrplot)
library(reshape2)
  
clean_data <- read.csv(here::here("dataset/clean_data.csv"))
clean_data$city_population_density <- as.integer(clean_data$city_population_density)

set.seed(2022)
split <- sample.split(clean_data, SplitRatio = 0.8)
dt = sort(sample(nrow(clean_data), nrow(clean_data)*.8))
train<-clean_data[dt,]
test<-clean_data[-dt,]

logistic_ispr<- glm(international_students_natlib~city_population+city_population_density+selectivity+size_qs+research_output_qs, train, family=binomial(), na.action=na.omit)
#summary(logistic_ispr)
logistic_ispo<- glm(as.factor(international_students_max)~city_population+city_population_density+selectivity+size_qs+research_output_qs, train, family=binomial(), na.action=na.omit)
#summary(logistic_ispo)
igauss_ispr<- glm(international_students_natlib~city_population+city_population_density+selectivity+size_qs+research_output_qs, train, family=inverse.gaussian(), na.action=na.omit)
#summary(igauss_ispr)
igauss_ispo<- glm(international_students_max~city_population+city_population_density+selectivity+size_qs+research_output_qs, train, family=inverse.gaussian(), na.action=na.omit)
#summary(igauss_ispo)

extracted_test_ispr<- select(test, city_population, city_population_density,selectivity,  size_qs, research_output_qs, international_students_natlib)
extracted_test_ispo<- select(test, city_population, city_population_density,selectivity,  size_qs, research_output_qs, international_students_max)

y_pred_logit_ispr <- data.frame(city_population= extracted_test_ispr$city_population,city_population_density = extracted_test_ispr$city_population_density,selectivity = extracted_test_ispr$selectivity,size_qs=extracted_test_ispr$size_qs,research_output_qs=extracted_test_ispr$research_output_qs, y =predict(logistic_ispr, extracted_test_ispr, type = "response"))
y_pred_logit_ispo <- data.frame(city_population= extracted_test_ispo$city_population,city_population_density = extracted_test_ispo$city_population_density,selectivity = extracted_test_ispo$selectivity,size_qs=extracted_test_ispo$size_qs,research_output_qs=extracted_test_ispo$research_output_qs, y =predict(logistic_ispo, extracted_test_ispo, type = "response"))
y_pred_igauss_ispr <- data.frame(city_population= extracted_test_ispr$city_population,city_population_density = extracted_test_ispr$city_population_density,selectivity = extracted_test_ispr$selectivity,size_qs=extracted_test_ispr$size_qs,research_output_qs=extracted_test_ispr$research_output_qs, y =predict(igauss_ispr, extracted_test_ispr, type = "response"))
y_pred_igauss_ispo <- data.frame(city_population= extracted_test_ispo$city_population,city_population_density = extracted_test_ispo$city_population_density,selectivity = extracted_test_ispo$selectivity,size_qs=extracted_test_ispo$size_qs,research_output_qs=extracted_test_ispo$research_output_qs, y =predict(igauss_ispo, extracted_test_ispo, type = "response"))

#International Student Proportion; Logistic Regression with predictors: (city_population,city_population_density,selectivity,size_qs,research_output_qs)
logit_result_ispr<- ggplot(extracted_test_ispr, aes(city_population,city_population_density,selectivity,size_qs,research_output_qs, y = extracted_test_ispr$international_students_natlib)) + geom_point() + geom_smooth(data = y_pred_logit_ispr,  se=FALSE, color='red4', alpha = 0.1)+
  theme_minimal() +
  labs(x='Predictors', y='International Student Proportions', title='Logistic Regression for International Student Proportion') +
  theme(plot.title = element_text(hjust=0.5, size=10, face='bold')) 
#logit_result_ispr

errors_logit_ispr <- extracted_test_ispr$international_students_natlib - y_pred_logit_ispr$y
mae_logit_ispr <- mean(abs(na.omit(errors_logit_ispr)))

#International Student Population; Logistic Regression with predictors: (city_population,city_population_density,selectivity,size_qs,research_output_qs)
logit_result_ispo<- ggplot(extracted_test_ispo, aes(city_population,city_population_density,selectivity,size_qs,research_output_qs, y = extracted_test_ispo$international_students_max)) + geom_point() + geom_smooth(data = y_pred_logit_ispo,  se=FALSE, color='red4', alpha = 0.1)+
  theme_minimal() +
  labs(x='Predictors', y='Population', title='Logistic Regression for International Student Population') +
  theme(plot.title = element_text(hjust=0.5, size=10, face='bold')) 
#logit_result_ispo

errors_logit_ispo <- extracted_test_ispo$international_students_max - y_pred_logit_ispo$y
mae_logit_ispo <- mean(abs(na.omit(errors_logit_ispo)))

#International Student Proportion; Inverse Gaussian Regression with predictors: (city_population,city_population_density,selectivity,size_qs,research_output_qs)
igauss_result_ispr<- ggplot(extracted_test_ispr, aes(city_population,city_population_density,selectivity,size_qs,research_output_qs, y = extracted_test_ispr$international_students_natlib)) + geom_point() + geom_smooth(data = y_pred_igauss_ispr,  se=FALSE, color='red4', alpha = 0.1)+
  theme_minimal() +
  labs(x='Predictors', y='Population', title='Inverse Gaussian Regression for International Student Proportion') +
  theme(plot.title = element_text(hjust=0.5, size=10, face='bold')) 
#logit_result_ispo

errors_igauss_ispr <- extracted_test_ispr$international_students_natlib - y_pred_igauss_ispr$y
mae_igauss_ispr <- mean(abs(na.omit(errors_igauss_ispr)))

#International Student Population; Inverse Gaussian Regression with predictors: (city_population,city_population_density,selectivity,size_qs,research_output_qs)
igauss_result_ispo<- ggplot(extracted_test_ispo, aes(city_population,city_population_density,selectivity,size_qs,research_output_qs, y = extracted_test_ispo$international_students_max)) + geom_point() + geom_smooth(data = y_pred_igauss_ispo,  se=FALSE, color='red4', alpha = 0.1)+
  theme_minimal() +
  labs(x='Predictors', y='Population', title='Logistic Regression for International Student Population') +
  theme(plot.title = element_text(hjust=0.5, size=10, face='bold')) 
#logit_result_ispo

errors_igauss_ispo <- extracted_test_ispo$international_students_max - y_pred_igauss_ispo$y
mae_igauss_ispo <- mean(abs(na.omit(errors_igauss_ispo)))

print("Mean Absolute Error of Logistic Regression for ISPr:")
print(mae_logit_ispr)
print("Mean Absolute Error of Logistic Regression for ISPo:")
print(mae_logit_ispo)
print("Mean Absolute Error of Inverse Gaussian Regression for ISPr:")
print(mae_igauss_ispr)
print("Mean Absolute Error of Inverse Gaussian Regression for ISPo:")
print(mae_igauss_ispo)
})
```
For evaluating performance on these two generalized linear models, we compute for mean absolute error (MAE) on the new test set with 89 examples.  
<br>
For international student population, the inverse Gaussian regression has lower MAE and performs better than the logistic regression. Conversely, for international student proportion, the logistic regression has lower MAE and performs better. 


#### Stepwise linear regression models:   
```{r, echo=FALSE, message = FALSE,  warning= FALSE}
#Stepwise regressions
#install.packages("leaps")
suppressWarnings({ 
library("leaps")
max_vars_ispr_forward <- 5
max_vars_ispr_backward <- 5
max_vars_ispo_forward <- 5
max_vars_ispo_backward <- 5
#Populating stepwise regression models for international student population and 
#proportion using forward and backward selection, respectively. 

#Notice that for the sake of minimizing loss of data from NA values, 
#we only include the variables that we believe are the best predictors
#city_population,city_population_density,selectivity+size_qs,research_output_qs.
stepwise_lms_ispr_forward <- regsubsets(international_students_natlib~city_population+city_population_density+selectivity+size_qs+research_output_qs, data=train, nvmax=max_vars_ispr_forward,method=c("forward"))
stepwise_lms_ispr_backward <- regsubsets(international_students_natlib~city_population+city_population_density+selectivity+size_qs+research_output_qs, data=train, nvmax=max_vars_ispr_backward,method=c("backward"))
stepwise_lms_ispo_forward <- regsubsets(international_students_max~city_population+city_population_density+selectivity+size_qs+research_output_qs, data=train, nvmax=max_vars_ispo_forward,method=c("forward"))
stepwise_lms_ispo_backward <- regsubsets(international_students_max~city_population+city_population_density+selectivity+size_qs+research_output_qs, data=train, nvmax=max_vars_ispo_backward,method=c("backward"))

#Stepwise regression, international student proportion, with forward selection
sum_lms_ispr_forward <- summary(stepwise_lms_ispr_forward)
data.frame(
  Adj.R2 = sum_lms_ispr_forward$adjr2,
  CP = sum_lms_ispr_forward$cp,
  BIC = sum_lms_ispr_forward$bic
)
a_ispr_forward <- which.max(sum_lms_ispr_forward$adjr2)
colnames(sum_lms_ispr_forward$which)[sum_lms_ispr_forward$which[a_ispr_forward,]]
b_ispr_forward <- which.min(sum_lms_ispr_forward$cp)
colnames(sum_lms_ispr_forward$which)[sum_lms_ispr_forward$which[b_ispr_forward,]]
c_ispr_forward <- which.min(sum_lms_ispr_forward$cp)
colnames(sum_lms_ispr_forward$which)[sum_lms_ispr_forward$which[c_ispr_forward,]]
#Best model according to adjusted R2, BIC, Mallow's CP
ispr_forward_best_lm<- lm(international_students_natlib ~ city_population+city_population_density+selectivity+size_qs+research_output_qs, train)

#Stepwise regression, international student proportion, with backward selection
sum_lms_ispr_backward <- summary(stepwise_lms_ispr_backward)
data.frame(
  Adj.R2 = sum_lms_ispr_backward$adjr2,
  CP = sum_lms_ispr_backward$cp,
  BIC = sum_lms_ispr_backward$bic
)
a_ispr_backward <- which.max(sum_lms_ispr_backward$adjr2)
colnames(sum_lms_ispr_backward$which)[sum_lms_ispr_backward$which[a_ispr_backward,]]
b_ispr_backward <- which.min(sum_lms_ispr_backward$cp)
colnames(sum_lms_ispr_backward$which)[sum_lms_ispr_backward$which[b_ispr_backward,]]
c_ispr_backward <- which.min(sum_lms_ispr_backward$cp)
colnames(sum_lms_ispr_backward$which)[sum_lms_ispr_backward$which[c_ispr_backward,]]
#Best model according to adjusted R2, BIC, Mallow's CP
ispr_backward_best_lm<-lm(international_students_natlib ~ city_population+city_population_density+selectivity+size_qs+research_output_qs, train)

#Stepwise regression, international student population, with forward selection
sum_lms_ispo_forward <- summary(stepwise_lms_ispo_forward)
data.frame(
  Adj.R2 = sum_lms_ispo_forward$adjr2,
  CP = sum_lms_ispo_forward$cp,
  BIC = sum_lms_ispo_forward$bic
)
a_ispo_forward <- which.max(sum_lms_ispo_forward$adjr2)
colnames(sum_lms_ispo_forward$which)[sum_lms_ispo_forward$which[a_ispo_forward,]]
b_ispo_forward <- which.min(sum_lms_ispo_forward$cp)
colnames(sum_lms_ispo_forward$which)[sum_lms_ispo_forward$which[b_ispo_forward,]]
c_ispo_forward <- which.min(sum_lms_ispr_forward$cp)
colnames(sum_lms_ispo_forward$which)[sum_lms_ispo_forward$which[c_ispo_forward,]]
#Best model according to adjusted R2, BIC, Mallow's CP
ispo_forward_best_lm<- lm(international_students_max ~ city_population_density+size_qs+research_output_qs, train)

#Stepwise regression, international student population, with backward selection
sum_lms_ispo_backward <- summary(stepwise_lms_ispo_backward)
data.frame(
  Adj.R2 = sum_lms_ispo_backward$adjr2,
  CP = sum_lms_ispo_backward$cp,
  BIC = sum_lms_ispo_backward$bic
)
a_ispo_backward <- which.max(sum_lms_ispo_backward$adjr2)
colnames(sum_lms_ispo_backward$which)[sum_lms_ispo_backward$which[a_ispo_backward,]]
b_ispo_backward <- which.min(sum_lms_ispo_backward$cp)
colnames(sum_lms_ispo_backward$which)[sum_lms_ispo_backward$which[b_ispo_backward,]]
c_ispo_backward <- which.min(sum_lms_ispo_backward$cp)
colnames(sum_lms_ispo_backward$which)[sum_lms_ispo_backward$which[c_ispo_backward,]]
#Best model according to adjusted R2, BIC, Mallow's CP
ispo_backward_best_lm<- lm(international_students_max ~ city_population_density+size_qs+research_output_qs, train)
})
#Notice that for international student proportion, 
#the best models from forward and backward are identical
#and the predictors are city_population,city_population_density,selectivity,size_qs,research_output_qs.

#Notice that for international student population, 
#the best models from forward and backward are identical 
#and the predictors are city_population_density, size_qs,research_output_qs.
summary(ispr_forward_best_lm)
summary(ispr_backward_best_lm)
summary(ispo_forward_best_lm)
summary(ispo_backward_best_lm)
```
For evaluating stepwise linear regression, we are interested in finding models with the high adjusted R2 Score, low Bayesian Information Criterion, and low Mallow's Cp.  
<br>
Notice that for ISPr, the best models from forward and backward are identical and the best predictors are city_population,city_population_density,selectivity,size_qs, and research_output_qs.  
<br>
Similarly, for ISPo, the best models from forward and backward are identical and the predictors are city_population_density, size_qs, and research_output_qs.  
<br>
We also see that the p-value scores are low for all models, which implies the predictors are statistically significant and are good indicators of prediction. Thus, we reject the null hypothesis and claim that we have identified good indicators of prediction of international student proportion and population.

## Conclusion
Our project concerns international student population and proportion of US colleges from 441 observations. Our analysis suggests that the city population, city population density, college selectivity, campus size, and faculty research output are good indicators for predicting international student population and international student proportion.

# Flaws and limitations
## Initial Limitation 1 (Resolved). 
One of the main limitations is that for our models we don’t introduce any categorical variables. We address this in the Feature Extraction section. Alternatively, we are interested in leveraging the regsubsets subroutine in the leaps library to compare modeling with different variables, which provides performance criteria such as Adjusted R2 score, Bayesian Information Criterion, Mallow’s Cp. 

### Solution:   
We have done feature extraction using correlation matrix and logistic regression. We have also used regsubsets and the above metrics to find the most promising categorical and continuous predictors.
<br>

## Initial Limitation 2 (Resolved). 
On the other hand, due to the sparsity of the data, if we include certain predictors, the overall data for training and testing may shrink to 10 observations, which leads to not unreliable modeling and predictions.

### Solution:  
We have imputed and corrected values for variables such as city, state, city_population, and city_population_density. In doing so, we are now able to work with up to at most 441 observations depending on selected predictors and also . 
<br>

## Current Limitations 1 (For Future Work)
While we have successfully revised the data using sources such as the US Census Bureau and the Univeristy page, we also have took it from sources such as <a href ="https://worldpopulationreview.com/"> World Population Review</a> and <a href ="http://www.city-data.com/"> City Data</a>. We are uncertain how reliable these sources are. 

### Proposed Solution:  
To workaround this, the investigator could webscrape from more reputable sources and limit the number of dataset merges to lessen the missing values. 

## Current Limitations 2 (For Future Work)
Again due to data sparsity, we still were only able to explore a few simple features. All of the predictors that we used were not "feature engineered".   

### Proposed Solution:  
For the next investigation, data on longitude and latitude of the universities could be explored to see whether distance (Euclidean, Manhattan, Harvesine, Cosine) to any international hubs or attractions. In general, the investigator could see try to see if there are any more non-linear relations between ISPr/ISPo and some combination or transformation of independent variables. The investigator would also need to apply some regularization penalties if there is overfitting from the feature engineering.

---
title: Data analysis 2 - Modeling Fitting
author: Linshen Cai, William Fang, Yawei Huang, Zeshi Wu, Eunyeong Park
date: '2022-11-04'
slug: []
categories: []
tags: []
description: We discuss both going deeper and broader into our EDA and our initial thoughts and results for modeling
toc: yes
authors: []
series: []
lastmod: '2022-11-04T11:28:32-04:00'
featuredVideo: ~
featuredImage: ~
---


<div id="TOC">

</div>

<p>EDA 1:</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(modelr)
library(caTools)

#Import Data
cleaned_data &lt;- read.csv(file = &quot;full_data_leftjoinCarnegie_clean2.csv&quot;)
processed_data1 &lt;- na.omit(data.frame(y = cleaned_data$international_students_natlib, x= cleaned_data$alumni_arwu + cleaned_data$pub_arwu) )
split &lt;- sample.split(processed_data1, SplitRatio = 0.7)
dt = sort(sample(nrow(processed_data1), nrow(processed_data1)*.7))
train&lt;-processed_data1[dt,]
test&lt;-processed_data1[-dt,]

#Comparing different model 
linear_model &lt;- lm(y~x, data=train)
summary(linear_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = train)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.053974 -0.033939 -0.009667  0.020905  0.127796 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.0915834  0.0150672   6.078 3.67e-07 ***
## x           0.0002286  0.0001802   1.268    0.212    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.0423 on 40 degrees of freedom
## Multiple R-squared:  0.03867,    Adjusted R-squared:  0.01464 
## F-statistic: 1.609 on 1 and 40 DF,  p-value: 0.212</code></pre>
<pre class="r"><code>ggplot(test,aes(x, y)) +
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, se=TRUE, color=&quot;turquoise4&quot;) +
  theme_minimal() +
  labs(x=&quot;Alumni and Publications factor (additive)&quot;, y=&quot;International Student Proportions&quot;, title=&quot;Linear Regression Plot&quot;) +
  theme(plot.title = element_text(hjust=0.5, size=20, face=&quot;bold&quot;))</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/1-1.png" width="672" /></p>
<pre class="r"><code>g_model1 &lt;- glm(y~x, data=train, family = binomial()) #Logistic 1</code></pre>
<pre><code>## Warning in eval(family$initialize): non-integer #successes in a binomial glm!</code></pre>
<pre class="r"><code>g_model2 &lt;- glm(y~x, data=train, family = quasibinomial()) #Logistic 2
g_model3 &lt;- glm(y~x, data=train, family = gaussian()) # Identity
g_model4 &lt;- glm(y~x, data=train, family = poisson()) #log</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.240000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.130000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.160000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.100000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.130000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.150000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.140000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.160000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.080000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.100000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.140000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.100000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.060000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.120000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.060000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.110000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.160000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.110000

## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.110000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.080000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.100000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.070000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.090000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.140000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.130000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.050000

## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.050000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.110000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.100000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.050000

## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.050000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.120000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.170000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.060000

## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.060000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.120000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.110000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.090000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.200000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.060000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.120000</code></pre>
<pre><code>## Warning in dpois(y, mu, log = TRUE): non-integer x = 0.080000</code></pre>
<pre class="r"><code>g_model5 &lt;- glm(y~x, data=train, family = inverse.gaussian()) #1/mu^2
g_model6 &lt;- glm(y~x, data=train, family = Gamma()) #Inverse
summary(g_model1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = binomial(), data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.19470  -0.11494  -0.03172   0.06625   0.35995  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -2.277820   1.144416  -1.990   0.0465 *
## x            0.002285   0.013268   0.172   0.8633  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 0.75210  on 41  degrees of freedom
## Residual deviance: 0.72304  on 40  degrees of freedom
## AIC: 13.68
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>summary(g_model2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = quasibinomial(), data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.19470  -0.11494  -0.03172   0.06625   0.35995  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.277820   0.157221 -14.488   &lt;2e-16 ***
## x            0.002285   0.001823   1.253    0.217    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasibinomial family taken to be 0.0188735)
## 
##     Null deviance: 0.75210  on 41  degrees of freedom
## Residual deviance: 0.72304  on 40  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>summary(g_model3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = gaussian(), data = train)
## 
## Deviance Residuals: 
##       Min         1Q     Median         3Q        Max  
## -0.053974  -0.033939  -0.009667   0.020905   0.127796  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.0915834  0.0150672   6.078 3.67e-07 ***
## x           0.0002286  0.0001802   1.268    0.212    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.001789047)
## 
##     Null deviance: 0.074440  on 41  degrees of freedom
## Residual deviance: 0.071562  on 40  degrees of freedom
## AIC: -142.55
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<pre class="r"><code>summary(g_model4)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = poisson(), data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.18634  -0.10937  -0.03002   0.06243   0.33201  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -2.373498   1.079182  -2.199   0.0279 *
## x            0.002025   0.012452   0.163   0.8708  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 0.66749  on 41  degrees of freedom
## Residual deviance: 0.64168  on 40  degrees of freedom
## AIC: Inf
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>summary(g_model5)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = inverse.gaussian(), data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3234  -1.0904  -0.2814   0.5349   2.3720  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 109.1961    23.6952   4.608 4.09e-05 ***
## x            -0.3109     0.2512  -1.238    0.223    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for inverse.gaussian family taken to be 1.510447)
## 
##     Null deviance: 65.105  on 41  degrees of freedom
## Residual deviance: 63.051  on 40  degrees of freedom
## AIC: -146.81
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>summary(g_model6)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = Gamma(), data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.65282  -0.35130  -0.09318   0.18222   0.87938  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.58208    1.29508   8.171 4.64e-10 ***
## x           -0.01782    0.01437  -1.240    0.222    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Gamma family taken to be 0.1595138)
## 
##     Null deviance: 6.3850  on 41  degrees of freedom
## Residual deviance: 6.1543  on 40  degrees of freedom
## AIC: -147.16
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>y_predict1 &lt;- data.frame(x = test$x, y =predict(g_model1, test, type = &quot;response&quot;))
y_predict2 &lt;- data.frame(x = test$x, y =predict(g_model2, test, type = &quot;response&quot;))
y_predict3 &lt;- data.frame(x = test$x, y =predict(g_model3, test, type = &quot;response&quot;))
y_predict4 &lt;- data.frame(x = test$x, y =predict(g_model4, test, type = &quot;response&quot;))
y_predict5 &lt;- data.frame(x = test$x, y =predict(g_model5, test, type = &quot;response&quot;))
y_predict6 &lt;- data.frame(x = test$x, y =predict(g_model6, test, type = &quot;response&quot;))
#Plotting graphs
g1&lt;- ggplot(test, aes(x, y)) + geom_point() + geom_smooth(data = y_predict1,  se=FALSE, color=&quot;red4&quot;, alpha = 0.1)+
  theme_minimal() +
  labs(x=&quot;Alumni and Publications factor (additive)&quot;, y=&quot;International Student Proportions&quot;, title=&quot;Logistic Regression&quot;) +
  theme(plot.title = element_text(hjust=0.5, size=20, face=&quot;bold&quot;))
g2&lt;- ggplot(test, aes(x, y)) + geom_point() + geom_smooth(data = y_predict2,  se=FALSE, color=&quot;orange4&quot;, alpha = 0.1)+
  theme_minimal() +
  labs(x=&quot;Alumni and Publications factor (additive)&quot;, y=&quot;International Student Proportions&quot;, title=&quot;Quasipolynomial Regression&quot;) +
  theme(plot.title = element_text(hjust=0.5, size=20, face=&quot;bold&quot;))
g3&lt;- ggplot(test, aes(x, y)) + geom_point() + geom_smooth(data = y_predict3,  se=FALSE, color=&quot;yellow4&quot;, alpha = 0.1)+
  theme_minimal() +
  labs(x=&quot;Alumni and Publications factor (additive)&quot;, y=&quot;International Student Proportions&quot;, title=&quot;Gaussian Regression (link: Identity Function) &quot;) +
  theme(plot.title = element_text(hjust=0.5, size=20, face=&quot;bold&quot;))
g4&lt;- ggplot(test, aes(x, y)) + geom_point() + geom_smooth(data = y_predict4,  se=FALSE, color=&quot;green4&quot;, alpha = 0.1)+
  theme_minimal() +
  labs(x=&quot;Alumni and Publications factor (additive)&quot;, y=&quot;International Student Proportions&quot;, title=&quot;Poisson Regression (link: Log Function)&quot;) +
  theme(plot.title = element_text(hjust=0.5, size=20, face=&quot;bold&quot;))
g5&lt;- ggplot(test, aes(x, y)) + geom_point() + geom_smooth(data = y_predict5,  se=FALSE, color=&quot;blue4&quot;, alpha = 0.1)+
  theme_minimal() +
  labs(x=&quot;Alumni and Publications factor (additive)&quot;, y=&quot;International Student Proportions&quot;, title=&quot;Inverse Gaussian Regression (link: 1/mu -squared Function)&quot;) +
  theme(plot.title = element_text(hjust=0.5, size=20, face=&quot;bold&quot;))
g6&lt;- ggplot(test, aes(x, y)) + geom_point() +  geom_smooth(data = y_predict6,  se=FALSE, color=&quot;purple4&quot;, alpha = 0.1)+
  theme_minimal() +
  labs(x=&quot;Alumni and Publications factor (additive)&quot;, y=&quot;International Student Proportions&quot;, title=&quot;Gamma Regression (link: Inverse Function)&quot;) +
  theme(plot.title = element_text(hjust=0.5, size=20, face=&quot;bold&quot;))
g1</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/1-2.png" width="672" /></p>
<pre class="r"><code>g5</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/1-3.png" width="672" /></p>
<p>This EDA concerns the relationship between dependent variable “International Student Proportions” and the independent variable, an additive indicator reflecting the number of “Alumni of an institution winning Nobel Prizes and Fields Medals” and “Papers indexed in Science Citation Index-Expanded and Social Science Citation Index”. We shuffled and split the corresponding data into respective 70% and 30% portions for training and test data. Then, we applied vanilla linear regression (lm) and 6 types of glm regression (logistic, quasipolynomial, Gaussian, Poisson, Inverse Gaussian, Gamma); they yield different results about International student proportion and the indicator, negative correlation and positive correlation respectively. All models are very similar because they use the generalized linear model formulation (log(E[Y|x]) where Y is the distribution output and the vector x captures the data points. Since the 6 GLMs have 6 different distributions and we use the same data points, it leads to varying penalties and similar best line of fit.</p>
<p>While it is difficult to compare the visualization to find the best model, we consider AIC and fisher score. AIC (Akaike information criterion) estimates the prediction error and model’s relative quality with formula AIC = 2K – 2ln(L), where K is the number of model parameters and ln(L) log likelihood of the model. Since the model with the lowest AIC provides the best fit, we select the Inverse Gaussian regression model with the link function 1/mu^2 and AIC of -140.97, which is around 2 units better than Gamma Regression model. Next consider that, Fisher Score is the gradient of the log likelihood of the model, which is used to solve Maximum Likelihood Estimation (MLE), with formula, u(θ)=∇_θ logp(x|θ). R’s glm method by default uses the iterative “Newton-Raphson algorithm” to estimate the model fit and “number of fisher scoring iterations” corresponds to which iteration leads to the convergence. The greatest number iterations is 6. At the end,we pick 3 models which are linear, logistic, and the inverse gaussian models.</p>
<p>Note:_θ just denotes θ as the parameter</p>
<p>EDA 2:</p>
<pre class="r"><code># Read in data and some basic cleaning
data_cleaned &lt;- read.csv(&quot;full_data_leftjoinCarnegie_clean2.csv&quot;)
y = data_cleaned$international_students_natlib
x1 = data_cleaned$n_s_arwu
x2 = data_cleaned$stu_facl_ratio_qs
#linear regression
linearRegression = lm(x1~x2)
summary(linearRegression)</code></pre>
<pre><code>## 
## Call:
## lm(formula = x1 ~ x2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -24.177 -11.104  -2.402   7.169  65.120 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  43.7172     3.7314  11.716  &lt; 2e-16 ***
## x2           -1.7675     0.3019  -5.854 7.51e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.45 on 91 degrees of freedom
##   (348 observations deleted due to missingness)
## Multiple R-squared:  0.2736, Adjusted R-squared:  0.2656 
## F-statistic: 34.27 on 1 and 91 DF,  p-value: 7.506e-08</code></pre>
<pre class="r"><code>#Plotting
data_cleaned %&gt;% ggplot(aes(x = n_s_arwu, y= stu_facl_ratio_qs)) +
  geom_point()+
  geom_smooth(method = &quot;lm&quot;,se = FALSE) +
  labs(x = &quot;Papers published in Nature and Science&quot;,
       y = &quot;Student-Faculty Ratio&quot;)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<pre><code>## Warning: Removed 348 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>## Warning: Removed 348 rows containing missing values (geom_point).</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/2-1.png" width="672" /></p>
<p>The EDA concerns the linear regression relationship between response variable “Papers published in Nature and Science” and predictor variable “Student Faculty Ratio”.There appears to be a general downward trend in relative student-faculty ratio as the number of paper published rises. The graph mainly shows the negative linear relation with slope coefficient -1.78 and intercept 43.21. The parameter estimate was 43.71with 91degrees of freedom and a p-value of 7.506e-08. Since this p-value is more than 0.05, we can’t reject the null hypothesis and conclude that there is not a significant linear association between the number of papers and Student Faculty Ratio.</p>
<p>Modeling:</p>
<pre class="r"><code># Read in data and some basic cleaning
fulldata &lt;- read_csv(&quot;full_data_leftjoinCarnegie_clean2.csv&quot;)</code></pre>
<pre><code>## Rows: 441 Columns: 37
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## chr  (9): university, city, state, world_rank_cat_arwu, world_rank_qs, natio...
## dbl (28): city_population_density, city_population, city_land_area, internat...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code>fulldata &lt;- fulldata %&gt;% 
  mutate(type1_all = ifelse(type1_all==3, 2, type1_all)) %&gt;% 
  mutate(type1_all=factor(type1_all))
## multivariate linear regression
mod &lt;- lm(international_students_max ~ 
            tuition_kaggle+
            world_rank_arwu+
            size_qs+
            type1_all, 
          data = fulldata)
summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = international_students_max ~ tuition_kaggle + world_rank_arwu + 
##     size_qs + type1_all, data = fulldata)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3796.5 -1611.0  -347.7   715.9  8580.1 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      2.952e+03  2.003e+03   1.474 0.145159    
## tuition_kaggle   4.787e-02  4.085e-02   1.172 0.245240    
## world_rank_arwu -1.079e+01  3.078e+00  -3.505 0.000807 ***
## size_qsM        -2.524e+03  8.886e+02  -2.840 0.005920 ** 
## size_qsS        -3.770e+03  1.828e+03  -2.062 0.042951 *  
## size_qsXL        3.593e+03  7.610e+02   4.722 1.19e-05 ***
## type1_all2       5.281e+02  1.041e+03   0.507 0.613562    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2465 on 69 degrees of freedom
##   (365 observations deleted due to missingness)
## Multiple R-squared:  0.5496, Adjusted R-squared:  0.5105 
## F-statistic: 14.03 on 6 and 69 DF,  p-value: 2.292e-10</code></pre>
<pre class="r"><code>## Plotting
fulldata %&gt;%
  mutate(tuition_kaggle = mean(tuition_kaggle, na.rm = TRUE)) %&gt;% 
  add_predictions(mod) %&gt;% 
  ggplot(aes(x=world_rank_arwu)) +
  geom_point(aes(y = international_students_max,
                 color=size_qs))+
  geom_line(aes(y = pred, 
                color=size_qs, 
                linetype = type1_all), 
            size = 1)+
  labs(
       x=&quot;ARWU World Ranking&quot;,
       y=&quot;Number of international students&quot;,
       color=&quot;campus size&quot;,
       linetype = &quot;public vs private&quot;)+
  theme_minimal()</code></pre>
<pre><code>## Warning: Removed 347 rows containing missing values (geom_point).</code></pre>
<pre><code>## Warning: Removed 348 row(s) containing missing values (geom_path).</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Codes:
mod &lt;- lm(international_students_max ~ tuition_kaggle + world_rank_arwu + size_qs + factor(type1_all), data = fulldata)</p>
<p>For modeling, we are interested in what predicts number of international students in US universities. We picked six predictor variables initially: out-of-state tuition, arwu world ranking, campus size, public vs private, selectvity, and city population. We fittted bivariate linear models for each of the six potential predictors and found that four of them (out-of-state tuition, arwu world ranking, campus size, and public vs private) significantly predict our outcome. We then put the four predictors in a multivariate linear model and got the below results. After controlling for one another, arwu world ranking and campus size remain to be significant predictor for international population. More specifically, after controlling for out-of-state tuition, campus size, public vs private, selectvity, and city population, there is still a significant relationship between world ranking and number of international students in that the higher ranking the university is, the more international studnets it has. In addition, after controlling for out-of-state tuition, arwu world ranking, public vs private, selectvity, and city population, there is still a significant relationship between campus size (Small, Median, Large, and Extra Large) and number of international students. The larger the campus is, the more international students it has.</p>
<p>Note that one challenge with the current model is that the number of observations decreased greatly due to missing values.</p>
<p>blogdown::serve_site()</p>
